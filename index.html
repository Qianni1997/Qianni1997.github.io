<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="It is a place for Qianni to record and learn.">
<meta property="og:type" content="website">
<meta property="og:title" content="Find Qianni">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Find Qianni">
<meta property="og:description" content="It is a place for Qianni to record and learn.">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Find Qianni">
<meta name="twitter:description" content="It is a place for Qianni to record and learn.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Find Qianni</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Find Qianni</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Always be Brave and Enthusiastic</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/ML-Cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/ML-Cluster/" itemprop="url">机器学习之聚类算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-05T17:26:02+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习之聚类算法-1"><a href="#机器学习之聚类算法-1" class="headerlink" title="机器学习之聚类算法(1)"></a>机器学习之聚类算法(1)</h1><h2 id="1-聚类的概念-Clustering"><a href="#1-聚类的概念-Clustering" class="headerlink" title="1. 聚类的概念(Clustering)"></a>1. 聚类的概念(Clustering)</h2><h3 id="1-1-聚类的概念"><a href="#1-1-聚类的概念" class="headerlink" title="1.1 聚类的概念"></a>1.1 聚类的概念</h3><p>机器学习按照学习形式可以分为“监督学习”与“无监督学习”，在无监督学习中，训练样本的标记信息未知，即<strong>没有“标签”</strong>，需要通过对无标记训练样本的学习来揭示数据的内在性质与规律，无监督学习任务中研究最多、应用最广的是聚类。</p>
<p>聚类将数据集中相似的样本归到同一个簇中，将不相似的地样本归到不同的簇中，每个簇可能对应于一些潜在的概念（类别），但这些概念对聚类算法而言事先是未知的。这其中的一系列概念可以表示为：</p>
<p>假定样本集$D=\left\{x_1,x_2,\cdots,x_m \right\}$包含$m$个无标记样本，每个样本$x_i=\left\{x_{i1};x_{i2};\cdots;x_{in} \right\}$是一个$n$维特征向量，则聚类算法将样本集$D$划分为$k$个不相交的簇$\left\{C_l | l=1,2,\cdots,k \right\}$,其中，$C_{l’}\cap_{l’\neq l}C_l=\varnothing $且$D=\cup_{l=1}^kC_l$，相应地，我们用$\lambda_j\in\left\{1,2,\cdots,k\right\}$表示样本$x_j$的簇标记，即$x_j\in C_{\lambda_j}$于是，聚类的结果可用包含$m$个元素的簇标记向量$\lambda=\left(\lambda_1;\lambda_2; \cdots;\lambda_m\right)$表示。</p>
<h3 id="1-2-聚类中的性能度量"><a href="#1-2-聚类中的性能度量" class="headerlink" title="1.2 聚类中的性能度量"></a>1.2 聚类中的性能度量</h3><p>聚类分析试图将相似对象划为同一簇，不相似的对象归到不同簇，而“相似”主要依靠性能度量。</p>
<p>聚类的性能度量（有效性指标）是用于评估聚类效果的好坏的或作为聚类过程的优化目标的。聚类性能度量大致可以分为两类，一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”。</p>
<p>&lt; !—more—&gt; </p>
<h4 id="1-2-1-外部指标"><a href="#1-2-1-外部指标" class="headerlink" title="1.2.1 外部指标"></a>1.2.1 外部指标</h4><p>对数据集$D=\left\{x_1,x_2,\cdots,x_m \right\}$，假定通过聚类给出的簇划分为$C=\left\{C_1,C_2,\cdots,C_k \right\}$参考模型给出的簇划分为$C^<em>=\left\{C_1^</em>,C_2^<em>,\cdots,C_s ^</em>\right\}$。相应地，令$\lambda$与$\lambda^<em>$分别表示$C$与$C^</em>$对应的簇标记向量，将样本两两配对考虑，则可定义：</p>
<script type="math/tex; mode=display">
a=\left|SS\right|, SS=\left\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
b=\left|SD\right|, SD=\left\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*\neq\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
c=\left|DS\right|,DS=\left\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda_i^*=\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
d=\left|DD\right|, SS=\left\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda_i^*\neq\lambda_j^*,i<j\right\}</script><p>其中集合$SS$为在$C$中隶属于相同簇且在$C^*$中也隶属于相同簇的样本对，其余同理可以推得，此外$a+b+c+d=\frac{m(m-1)}{2}$</p>
<p>通过以上定义得到聚类性能度量的外部指标：</p>
<ul>
<li>Jaccard系数 $JC=\frac{a}{a+b+c}$</li>
<li>FM指数 $FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}$</li>
<li>Rand指数 $RI=\frac{2(a+d)}{m(m-1)}$</li>
</ul>
<h4 id="1-2-2-内部指标"><a href="#1-2-2-内部指标" class="headerlink" title="1.2.2 内部指标"></a>1.2.2 内部指标</h4><p>根据簇划分$C=\left\{C_1,C_2,\cdots,C_k \right\}$，定义：</p>
<script type="math/tex; mode=display">
avg(C)=\frac{2}{\left|C\right|(\left|C\right| - 1)}\sum_{1\leq i<j\leq\left|C\right|}dist(x_i,x_j)</script><script type="math/tex; mode=display">
diam(C)=\max_{1\leq i<j\leq\left|C\right|}dist(x_i,x_j)</script><script type="math/tex; mode=display">
d_\min (C_i,C_j)=\min_{x_i\in C_i,x_j \in C_j}dist(x_i,x_j)</script><script type="math/tex; mode=display">
d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)</script><p>$dist$为样本距离计算公式，$\mu$代表簇$C$的中心点，$avf(C)$对应簇$C$内样本间的平均距离，$diam(C)$对应簇内$C$内样本间的最远距离，$d_\min (C_i,C_j)$为两个簇最近样本间的距离$d_{cen}(C_i,C_j)$为两簇中心点间的距离。</p>
<p>通过以上可以导出以下内部指标：</p>
<ul>
<li>DBI指数 $DBI =\frac{1}{k}\sum_{i=1}^k\max_{j\neq i}\left( \frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\right)$</li>
<li>Dunn指数 $DI=\min_{1\leq i \leq k}\left\{\min_{j\neq i}\left(\frac{d_\min(C_i,C_j)}{\max_{1\leq l \leq k}diam(C_l)}\right)\right\}$</li>
</ul>
<p>DBI的值越小越好，DI的值越大越好。</p>
<h3 id="1-3-聚类中的距离"><a href="#1-3-聚类中的距离" class="headerlink" title="1.3 聚类中的距离"></a>1.3 聚类中的距离</h3><p>性能度量中的内部指标涉及到“距离”的度量，数据的属性可以分为“有序属性”（如{1,2,3}可以直接衡量出各个量之间的距离）与“无序属性”（如{飞机，火车，轮船}不能直接刻画各个量之间的距离）。</p>
<p>其中<strong>“有序属性”</strong>可以通过<strong>闵可夫斯基距离</strong>计算：</p>
<script type="math/tex; mode=display">
dist_{mk}(x_i,x_j)=\left(\sum_{u=1}^n \left|x_{iu}-x_{ju}\right|^p\right)^\frac{1}{p}</script><p>p=1时，为曼哈顿距离；p=2时为欧式距离；此外当样本空间中不同属性的重要性不同时，可以使用“加权距离”进行计算。</p>
<p><strong>“无序属性”</strong>可以通过<strong>VDM</strong>(value Difference Metric)计算：</p>
<script type="math/tex; mode=display">
VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p</script><p>其中$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示在第$i$个样本簇中在属性$u$上取值为$a$的样本数，$k$为样本簇数。</p>
<p>对于<strong>同时存在“有序属性”与“无序属性”</strong>的情况，可以采用<strong>混合距离定义</strong>，其中有$n_c$个有序属性，$n-n_c$个无序属性：</p>
<script type="math/tex; mode=display">
MinkovDM_p(x_i,x_j)=\left( \sum_{u=1}^n \left|x_{iu}-x_{ju}\right|^p+ \sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\right)^\frac{1}{p}</script><p>距离度量需要满足如非负性、同一性、对称性、直递性（两边之和大于第三边）等性质，但相似度度量的过程中使用的距离未必一定要满足距离度量的所有基本性质。</p>
<h3 id="1-4-聚类的分类"><a href="#1-4-聚类的分类" class="headerlink" title="1.4 聚类的分类"></a>1.4 聚类的分类</h3><p>看过几个版本的聚类的分类方法，基本是一致的，但也有一点点差异，此处选取周志华《机器学习》一书中的分类方法。</p>
<ul>
<li><strong>原型聚类</strong>：假设聚类结构能通过一组<strong>原型</strong>刻画，算法一般先对原型进行初始化，然后对原型进行迭代更新求解。代表方法包括<strong>k-means</strong>；LVQ；<strong>GMM</strong>等。</li>
<li><strong>密度聚类</strong>：基于密度的方法的特点是不依赖于距离，而是<strong>依赖于密度</strong>，从而克服基于距离的算法只能发现“球形”聚簇的缺点。其核心思想在于只要一个区域中点的密度大于某个阈值，就把它加到与之相近的聚类中去。 通常情况，密度聚类算法从样本密度的角度来考察样本之间的<strong>可连接性</strong>，并基于<strong>可连接样本不断扩展聚类簇</strong>以获得最终的结果。代表方法包括<strong>DBSCAN</strong>、OPTICS，DENCLUE，WaveCluster等。</li>
<li><strong>层次聚类</strong>：试图在不同层次对数据集进行划分，从而形成树型的聚类结构，存在“自底而上”的聚合策略，与“自顶而下”的分拆策略。在“自底向上”方案中，初始时每个数据点组成一个单独的组，在接下来的迭代中，按一定的距离度量将相互邻近的组合并成一个组，直至所有的记录组成一个分组或者满足某个条件为止。 代表方法包括：AGNES、BIRCH，CURE，CHAMELEON等。</li>
</ul>
<h2 id="2-原型聚类——K-均值聚类算法-K-means"><a href="#2-原型聚类——K-均值聚类算法-K-means" class="headerlink" title="2.原型聚类——K-均值聚类算法(K-means)"></a>2.原型聚类——K-均值聚类算法(K-means)</h2><h3 id="2-1-过程及算法"><a href="#2-1-过程及算法" class="headerlink" title="2.1 过程及算法"></a>2.1 过程及算法</h3><p><img src="/images/ML_clustering/Kmeans.jpg" alt="Kmeans "></p>
<p>如上图所示，是K均值聚类的原理。对数据集$D=\left\{x_1,x_2,\cdots,x_m \right\}$，K-均值算法针对聚类所得簇划分$C=\left\{C_1,C_2,\cdots,C_k \right\}$最小化平方误差：</p>
<script type="math/tex; mode=display">
E=\sum_{i=1}^k \sum_{x\in C_i}\|x-\mu_i \|_2^2</script><p>其中k是用户给定的簇的个数，$\mu_i$是簇$C_i$的均值向量，即簇的“质心”。找到E的最小值是一个<a href="https://en.wikipedia.org/wiki/NP-hardness" target="_blank" rel="noopener">NP困难问题</a>，k-均值算法采用<a href="https://en.wikipedia.org/wiki/Greedy_algorithm" target="_blank" rel="noopener">贪心策略</a>，通过迭代优化近似求解，算法首先对于质心（均值向量）进行初始化，随机确定k个初始点作为质心，然后将数据集中的每个点寻找距离其最近的质心，并将该点分给该质心代表的簇，之后更新簇的质心（均值向量）为该簇现有所有点的平均值。</p>
<p>伪代码过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为起始质心（可以在样本中随机选择k个）</span><br><span class="line">当任意一个点的簇分配结果发生改变时</span><br><span class="line">	对数据集中的每个数据点</span><br><span class="line">		对每个质心</span><br><span class="line">			计算质心与数据点间的距离</span><br><span class="line">		将数据点分配到距离其最近的簇</span><br><span class="line">	对每个簇，计算簇中所有点的均值向量，并将值作为质心</span><br></pre></td></tr></table></figure>
<p><img src="/images/ML_clustering/KmeansAlgorithm.jpg" alt="KmeansA伪代码"></p>
<p>使用Python建立k-means过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#导入tab分隔文件</span></span><br><span class="line">    dataMat = []                <span class="comment">#assume last column is target value</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float,curLine)) <span class="comment">#此处map前加一个list才保证不出错</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span>       <span class="comment">#计算点之间的欧式距离</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment">#la.norm(vecA-vecB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span>        <span class="comment">#随机选取质心</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))<span class="comment">#create centroid mat</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment">#create random cluster centers, within bounds of each dimension</span></span><br><span class="line">        minJ = min(dataSet[:,j]) </span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span><span class="comment">#kmeans建立，输入数据集、簇个数</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]<span class="comment">#样本个数</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>)))<span class="comment">#存放聚类的类别和距离值</span></span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:<span class="comment">#当质心有所改变时持续循环</span></span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):<span class="comment">#对于每个数据点分配最近的质心</span></span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> (centroids)</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):<span class="comment">#更新质心</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]<span class="comment">#get all the point in this cluster</span></span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment">#assign centroid to mean </span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>
<h3 id="2-2-优缺点及适用性分析"><a href="#2-2-优缺点及适用性分析" class="headerlink" title="2.2 优缺点及适用性分析"></a>2.2 优缺点及适用性分析</h3><p>优点：便于实现</p>
<p>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</p>
<h2 id="3-二分K-均值算法-bisecting-K-means"><a href="#3-二分K-均值算法-bisecting-K-means" class="headerlink" title="3. 二分K-均值算法(bisecting K-means)"></a>3. 二分K-均值算法(bisecting K-means)</h2><p>由于传统的KMeans算法的聚类结果<strong>易受到初始聚类中心点选择的影响</strong>，因此在传统的KMeans算法的基础上进行算法改进，对初始中心点选取比较严格，各中心点的距离较远，这就避免了初始聚类中心会选到一个类上，一定程度上<strong>克服了算法陷入局部最优状态</strong>。 二分KMeans(Bisecting KMeans)算法的主要思想是：首先将<strong>所有点作为一个簇</strong>，然后将该簇<strong>一分为二</strong>。之后<strong>选择能最大限度降低聚类代价函数（也就是<code>误差平方和</code>SSE）的簇划分为两个簇</strong>。以此进行下去，<strong>直到簇的数目等于用户给定的数目k为止</strong>。以上隐含的一个原则就是：因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以我们就需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = np.mat(np.zeros((m,<span class="number">2</span>)))</span><br><span class="line">    centroid0 = np.mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">    centList =[centroid0] <span class="comment">#create a list with one centroid</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):<span class="comment">#calc initial Error</span></span><br><span class="line">        clusterAssment[j,<span class="number">1</span>] = distMeas(np.mat(centroid0), dataSet[j,:])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):</span><br><span class="line">        lowestSSE = np.inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]<span class="comment">#get the data points currently in cluster i</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)</span><br><span class="line">            sseSplit = sum(splitClustAss[:,<span class="number">1</span>])<span class="comment">#compare the SSE to the currrent minimum</span></span><br><span class="line">            sseNotSplit = sum(clusterAssment[np.nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"sseSplit, and notSplit: "</span>,sseSplit,sseNotSplit)</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = len(centList) <span class="comment">#change 1 to 3,4, or whatever</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the bestCentToSplit is: '</span>,bestCentToSplit)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the len of bestClustAss is: '</span>, len(bestClustAss))</span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>]<span class="comment">#replace a centroid with two best centroids </span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>])</span><br><span class="line">        clusterAssment[np.nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss<span class="comment">#reassign new clusters, and SSE</span></span><br><span class="line">    <span class="keyword">return</span> np.mat(centList), clusterAssment</span><br></pre></td></tr></table></figure>
<h2 id="4-K-均值算法实战：对地图上的点进行聚类"><a href="#4-K-均值算法实战：对地图上的点进行聚类" class="headerlink" title="4.K-均值算法实战：对地图上的点进行聚类"></a>4.K-均值算法实战：对地图上的点进行聚类</h2><p>这次没有选择《机器学习实战》课本上的案例，而是选取了2016.12.22周四00:00-00:30之间上海市出租车订单数据中所有正常数据的订单起始点进行聚类（总共137个数据点，数据来源是上学期交通地理信息系统课程）。此外还使用了上海市的路网shp文件，借助geopandas、numpy、matplotlib、pandas等模块，完成这些起始点的聚类，通过此过程我们可以看到热点地区的分布情况。</p>
<p>通过调用clusterOrders(numClust)即可进行进行以上聚类过程，此部分源代码如下，此外还需导入randCent()，kMeans()，biKmeans()函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distSLC</span><span class="params">(vecA, vecB)</span>:</span><span class="comment">#由经纬度转化为距离</span></span><br><span class="line">    a = np.sin(vecA[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * np.sin(vecB[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>)</span><br><span class="line">    b = np.cos(vecA[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * np.cos(vecB[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * \</span><br><span class="line">                      np.cos(np.pi * (vecB[<span class="number">0</span>,<span class="number">0</span>]-vecA[<span class="number">0</span>,<span class="number">0</span>]) /<span class="number">180</span>)</span><br><span class="line">    <span class="keyword">return</span> np.arccos(a + b)*<span class="number">6371.0</span> </span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd</span><br><span class="line">route=gpd.read_file(<span class="string">'SHMAP_Main.shp'</span>)<span class="comment">#导入路网shp文件</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clusterOrders</span><span class="params">(numClust=<span class="number">5</span>)</span>:</span>      <span class="comment">##对订单进行聚类</span></span><br><span class="line">    datList = []</span><br><span class="line">    data = pd.read_csv(<span class="string">'Thursdaymidnight.csv'</span>)  <span class="comment">##导入订单数据</span></span><br><span class="line">    location = data[[<span class="string">'s_long'</span>,<span class="string">'s_la'</span>]] <span class="comment">#使用订单的起点经纬度数据</span></span><br><span class="line">    inner=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(location.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">            a=location.iloc[i][j]</span><br><span class="line">            inner.append(a)</span><br><span class="line">        datList.append(inner)</span><br><span class="line">        inner=[]</span><br><span class="line">    datMat = np.mat(datList)    </span><br><span class="line">    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC) <span class="comment">#调用二分K-均值算法进行聚类</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">9</span>,<span class="number">15</span>))</span><br><span class="line">    rect=[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.8</span>,<span class="number">0.8</span>]</span><br><span class="line">    scatterMarkers=[<span class="string">'s'</span>, <span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'8'</span>, <span class="string">'p'</span>, \</span><br><span class="line">                    <span class="string">'d'</span>, <span class="string">'v'</span>, <span class="string">'h'</span>, <span class="string">'&gt;'</span>, <span class="string">'&lt;'</span>]</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax0=fig.add_axes(rect, label=<span class="string">'ax0'</span>, **axprops)</span><br><span class="line">    route.plot(ax=ax0,color=<span class="string">'darkgrey'</span>,alpha=<span class="number">0.2</span>)  <span class="comment">#绘制路网</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numClust):</span><br><span class="line">        ptsInCurrCluster = datMat[np.nonzero(clustAssing[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">        markerStyle = scatterMarkers[i % len(scatterMarkers)]</span><br><span class="line">        ax0.scatter(ptsInCurrCluster[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], ptsInCurrCluster[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=markerStyle, s=<span class="number">60</span>) <span class="comment">#绘制订单数据点</span></span><br><span class="line">    ax0.scatter(myCentroids[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], myCentroids[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=<span class="string">'+'</span>, s=<span class="number">300</span>,color=<span class="string">'k'</span>) <span class="comment">#绘制质心</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>尝试簇的数目由2至9，得到的聚类结果如下图所示：</p>
<p><img src="/images/ML_clustering/2.png" alt="Kmeans-1"><br><img src="/images/ML_clustering/3.png" alt="Kmeans-2"><br><img src="/images/ML_clustering/4.png" alt="Kmeans-3"><br><img src="/images/ML_clustering/5.png" alt="Kmeans-4"><br><img src="/images/ML_clustering/6.png" alt="Kmeans-5"><br><img src="/images/ML_clustering/7.png" alt="Kmeans-6"><br><img src="/images/ML_clustering/8.png" alt="Kmeans-7"><br><img src="/images/ML_clustering/9.png" alt="Kmeans-8"></p>
<p>此案例只应用了非常少量的一部分订单数据，在应用于量较大的订单数据时，我们可以不绘制各个订单起始点，而是将每一簇的数目与簇质心‘+’符号的尺寸产生关联，使热点区域的显示更为直观。</p>
<p>关于原型聚类中的GMM高斯混合模型聚类、密度聚类与层次聚类由于内容较多，推导较为复杂，本篇内就不再详细叙述，GMM大概会与EM算法一起写一篇吧！</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>周志华《机器学习》第9章<br>《机器学习实战》第10章<br>很多很多没有记录下来的博客</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/29/ML-1to6task/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/29/ML-1to6task/" itemprop="url">机器学习报告补齐（包括KNN、决策树、朴素贝叶斯、logistic回归、TensorFlow、SVM）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-29T15:39:48+08:00">
                2018-05-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习课程报告发布（分类部分）"><a href="#机器学习课程报告发布（分类部分）" class="headerlink" title="机器学习课程报告发布（分类部分）"></a>机器学习课程报告发布（分类部分）</h1><p>学期初，在自己的“对于知识的向往”和高中校友茅信的杜铭枢同学的鼓动下，报名了学校为茅院辅修大数据专业同学们开设的课程——机器学习与模式识别。课程鼓励对成果进行分享，因此在初期阶段我们进行了多次报告、课堂展示，而在课程中期，这一展示形式推广到了博客的方式。作为一个初学者，对于一些概念的掌握还十分浅显，再加之初期的doc格式报告不易转换成markdown文件，因此，此处选择转为pdf文件的形式对课程的前6次报告进行上传。过程中还有很多需要改进的地方，我也找到了进一步努力的方向。愿通过这门“戏很多”的课程，能够在数据分析、算法理解、报告撰写、课堂展示等方面有所收获。</p>
<p>下面按照课程要求，发布学习报告。</p>
<h2 id="2018-03-20-KNN算法"><a href="#2018-03-20-KNN算法" class="headerlink" title="2018-03-20 KNN算法"></a>2018-03-20 KNN算法</h2><p><strong>KNN算法工作原理</strong>：存在一个样本数据集合，也被称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最邻近）的分类标签。一般来说选取样本数据集中前k个最相似的数据，最后，选择k个最相似数据中出现次数最多的分类。</p>
<p><a href="/MLdownload/1KNN.pdf">点击下载KNN报告</a> </p>
<h2 id="2018-03-27-Decision-Tree决策树"><a href="#2018-03-27-Decision-Tree决策树" class="headerlink" title="2018-03-27 Decision Tree决策树"></a>2018-03-27 Decision Tree决策树</h2><p><strong>决策树</strong>算法是一种<code>逼近离散函数值</code>的方法。它是一种典型的分类方法，首先对<u>数据进行处理</u>，利用归纳算法生成可读的<u>规则</u>和<u>决策树</u>，然后使用决策树对新<u>数据进行分析</u>。本质上决策树是通过一系列规则对数据进行分类的过程。 </p>
<p><strong>决策树ID3算法原理</strong>：ID3算法是以<code>信息熵的下降速度</code>为选取测试属性的标准，即在每个节点选取<u>还尚未被用来划分</u>的<u>具有最高信息增益的属性</u>作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。ID3算法是决策树算法中较为简单的一项，此外还有C4.5、CART等决策树算法。</p>
<p><a href="/MLdownload/2Decisiontree.pdf">点击下载决策树报告</a> </p>
<p>&lt; !—more—&gt; </p>
<h2 id="2018-04-17-Naive-Bayes朴素贝叶斯"><a href="#2018-04-17-Naive-Bayes朴素贝叶斯" class="headerlink" title="2018-04-17 Naive Bayes朴素贝叶斯"></a>2018-04-17 Naive Bayes朴素贝叶斯</h2><p><strong>朴素贝叶斯法原理</strong>：朴素贝叶斯法是基于<u>贝叶斯定理</u>与<u>特征条件独立假设</u>的分类方法。 </p>
<p>其中贝叶斯定理可写作：</p>
<script type="math/tex; mode=display">
P(类别|特征)=\frac{P(特征|类别)P(类别)}{P(特征)}</script><script type="math/tex; mode=display">
P(B|A)=\frac{P(A|B)P(B)}{P(A)}</script><p>以上公式依照特征条件独立假设，可以进一步拆分为乘法，从而求算相关数据值进行分类。</p>
<p><a href="/MLdownload/3NaiveBayes.pdf">点击下载朴素贝叶斯报告</a> </p>
<h2 id="2018-04-24-Logistic回归"><a href="#2018-04-24-Logistic回归" class="headerlink" title="2018-04-24 Logistic回归"></a>2018-04-24 Logistic回归</h2><p><strong>利用Logistic回归进行分类的思想</strong>：根据现有数据对分类边界线建立回归公式，以此进行分类。回归的工作即为通过最优化方法(如梯度下降)找到最佳回归系数。</p>
<p><strong>具体操作步骤</strong>：在一系列数据中，对样本的每个数据特征乘某一个回归系数（某些时候加入一个sigmoid转换）并做累和，得到样本的分类函数$h(x)=h_\theta (x)=\theta_0+\theta_1 x_1+\cdots+\theta_n x_n$。将不同样本间分类函数与真实值间的误差的平方和进行累加，得到错误估计函数$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$ 。应用最优化方法求解错误估计函数最小时的回归系数$\theta_j:=\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$。此处的回归系数梯度下降是一个样本的状况，多个样本则需要进行如求和等处理。再依照此最佳回归系数建立Logistic回归，输出分类。</p>
<p>以上回归系数的确定步骤还可以依据<strong>梯度上升</strong>法，这种方法<strong>求最大值</strong>的核心思想是将自变量沿着目标函数的梯度方向移动，一直移动到指定的次数或者说某个允许的误差范围。 这一过程的伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每个回归系数初始化为1</span><br><span class="line">重复R次:</span><br><span class="line">     计算整个数据集的梯度</span><br><span class="line">     使用 alpha * gradient 更新回归系数向量</span><br><span class="line">返回回归系数</span><br></pre></td></tr></table></figure>
<p><a href="/MLdownload/4LogisticRegression.pdf">点击下载Logistic回归报告</a> </p>
<h2 id="2018-04-24-TensorFlow使用方法自主学习及实现Logistic回归"><a href="#2018-04-24-TensorFlow使用方法自主学习及实现Logistic回归" class="headerlink" title="2018-04-24 TensorFlow使用方法自主学习及实现Logistic回归"></a>2018-04-24 TensorFlow使用方法自主学习及实现Logistic回归</h2><p>TensorFlow是谷歌研发的第二代人工智能学习系统，可以用于进行机器学习、深度学习等工作。</p>
<p><a href="/MLdownload/5TensorFlow.pdf">点击下载报告</a> </p>
<h2 id="2018-05-08-SVM支持向量机"><a href="#2018-05-08-SVM支持向量机" class="headerlink" title="2018-05-08 SVM支持向量机"></a>2018-05-08 SVM支持向量机</h2><p><strong>SVM基本思想：</strong></p>
<p><strong>输入：</strong>两组带标签的数据。</p>
<p><strong>输出：</strong>一条<u>最优</u>的将两组数据分开的直线。</p>
<p>此过程也可以扩展到高维空间，即N维空间中，使用N-1维的最优超平面将不同标签的数据分开。SVM的数学证明过程较为严格，可以参考MIT的人工智能课程<a href="http://open.163.com/movie/2017/9/Q/2/MCTMNN3UI_MCTMP1NQ2.html" target="_blank" rel="noopener">http://open.163.com/movie/2017/9/Q/2/MCTMNN3UI_MCTMP1NQ2.html</a>，我也还需要进一步学习。</p>
<p><a href="/MLdownload/6SVM.pdf">点击下载SVM报告</a> </p>
<h2 id="2018-05-22-adaboost元算法"><a href="#2018-05-22-adaboost元算法" class="headerlink" title="2018-05-22 adaboost元算法"></a>2018-05-22 adaboost元算法</h2><p>[adaboost元算法博客地址]<a href="https://qianni1997.github.io/2018/05/26/ML-Adaboost/" target="_blank" rel="noopener">https://qianni1997.github.io/2018/05/26/ML-Adaboost/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/28/ML-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/28/ML-regression/" itemprop="url">机器学习之线性回归与局部加权线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-28T12:14:47+08:00">
                2018-05-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《机器学习实战》第8章-预测数值型数据：回归"><a href="#《机器学习实战》第8章-预测数值型数据：回归" class="headerlink" title="《机器学习实战》第8章 预测数值型数据：回归"></a>《机器学习实战》第8章 预测数值型数据：回归</h1><p>不知道算是第几次接触、学习回归，如果没记错的话：大一的建模课、熊熊的数据采集处理、在弃坑Andrew Ng的网课前……一直到本次的自学。对于一些问题理解的不深刻是我一直以来的问题，这个问题主要源于我对线性代数内容的不熟悉，对理论的推导过程不熟悉等等因素。本学期机器学习课程又一次接触回归问题，希望这一次可以做一个比较完整的梳理，以备以后学习结束后及时复习、查看~</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#本部分需要调用的库</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>内容提要：<br>线性回归<br>局部加权线性回归<br>预测鲍鱼年龄的实例</p>
</blockquote>
<h2 id="一、线性回归-LR"><a href="#一、线性回归-LR" class="headerlink" title="一、线性回归(LR)"></a>一、线性回归(LR)</h2><h3 id="1-线性回归含义解释"><a href="#1-线性回归含义解释" class="headerlink" title="1.线性回归含义解释"></a>1.线性回归含义解释</h3><p>线性回归是给定一系列点数据点，建立“<strong>最佳</strong>“”<strong>近似</strong>“”<strong>线性</strong>“关系。回归的<strong>目的</strong>是<u>预测数值型的目标值</u>，回归的过程就是求回归方程的回归系数。</p>
<ul>
<li>最佳：有不同的定义；如：找出使<strong>误差（预测y值与真实y值之间的差值）最小</strong>的回归系数，这里的误差一般采用<strong>平方误差</strong></li>
<li>近似：由于模型误差与测量误差的存在，不能完美地描述数据</li>
<li>线性：与线性代数存在一定关系</li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/28/ML-regression/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/26/ML-Adaboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/26/ML-Adaboost/" itemprop="url">机器学习之Adaboost元算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-26T11:55:48+08:00">
                2018-05-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《机器学习实战》第7章-利用AdaBoost元算法提高分类性能"><a href="#《机器学习实战》第7章-利用AdaBoost元算法提高分类性能" class="headerlink" title="《机器学习实战》第7章 利用AdaBoost元算法提高分类性能"></a>《机器学习实战》第7章 利用AdaBoost元算法提高分类性能</h1><p>最近的机器学习课程要求博客分享学习报告，因此借助博客进行分享，希望大家提出宝贵意见。此外由于还没有搞清楚hexo如何分栏目，所以暂未进行分栏，接下来熟悉后会对文章进行分栏等处理，保持博客的“清爽”。</p>
<h2 id="7-1-基于数据集多重抽样的分类器"><a href="#7-1-基于数据集多重抽样的分类器" class="headerlink" title="7.1 基于数据集多重抽样的分类器"></a>7.1 基于数据集多重抽样的分类器</h2><p>集成方法(<code>ensemble method</code>)又称为元算法(<code>meta-algorithm</code>)是对其他算法进行组合的一种方式，组合的方式主要有：</p>
<p>①不同的分类算法集成（KNN、决策树、朴素贝叶斯、Logistics、SVM等分类算法进行集成）<br>②同一算法在不同设置下集成（算法参数不同）<br>③数据集不同部分分配给不同分类器之后的集成</p>
<p>通常来讲一个集成分类器的分类性能将优于单个的分类器，单个分类器如果是一个决策者，集成学习的方法则为多决策者共同决策一个问题。 </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/26/ML-Adaboost/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/22/Lecture notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/22/Lecture notes/" itemprop="url">如何撰写国际同行审阅论文？讲座笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-22T12:14:47+08:00">
                2018-05-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="如何撰写国际同行审阅论文？讲座笔记"><a href="#如何撰写国际同行审阅论文？讲座笔记" class="headerlink" title="如何撰写国际同行审阅论文？讲座笔记"></a>如何撰写国际同行审阅论文？讲座笔记</h1><p>主讲人：曹新宇教授 （美国明尼苏达大学）</p>
<p>讲座时间地点：2018.5.15交运楼417会议室</p>
<p>主讲人及讲座介绍链接：<a href="http://ctt.swjtu.edu.cn/info/1041/2604.htm" target="_blank" rel="noopener">http://ctt.swjtu.edu.cn/info/1041/2604.htm</a></p>
<hr>
<h2 id="写在正式内容以前"><a href="#写在正式内容以前" class="headerlink" title="写在正式内容以前"></a>写在正式内容以前</h2><p>曹教授从编辑(Editor)，审稿人(Reviewer)与作者(Author)三个角度展开。首先，提出问题：如何审阅期刊文章？(How to Rewiew a Journal Article?)</p>
<p>针对此问题，曹教授引用Robert M. Terry所讲的Reviewer在审稿的短时间(可能仅有5-10min)内<strong>关注的几个问题</strong>，大家可以拿来对照论文参考。</p>
<ol>
<li>文章的<strong>写作目的</strong>。</li>
<li>文章所涉及的<strong>主题</strong>及文章<strong>创新点</strong>。</li>
<li><strong>研究方法</strong>及研究方法和评价的可靠性。</li>
<li>作者<strong>成果</strong>及成果是否叙述清晰。</li>
<li>作者成果如何<strong>推动该领域发展</strong>。</li>
</ol>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/22/Lecture notes/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/21/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/21/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-21T12:20:56+08:00">
                2018-05-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/portrait.jpg"
                alt="Wang Qianni" />
            
              <p class="site-author-name" itemprop="name">Wang Qianni</p>
              <p class="site-description motion-element" itemprop="description">It is a place for Qianni to record and learn.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Qianni</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
