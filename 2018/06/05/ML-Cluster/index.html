<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Clustering," />










<meta name="description" content="机器学习之聚类算法(1)1. 聚类的概念(Clustering)1.1 聚类的概念机器学习按照学习形式可以分为“监督学习”与“无监督学习”，在无监督学习中，训练样本的标记信息未知，即没有“标签”，需要通过对无标记训练样本的学习来揭示数据的内在性质与规律，无监督学习任务中研究最多、应用最广的是聚类。">
<meta name="keywords" content="Clustering">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之聚类算法">
<meta property="og:url" content="http://yoursite.com/2018/06/05/ML-Cluster/index.html">
<meta property="og:site_name" content="Find Qianni">
<meta property="og:description" content="机器学习之聚类算法(1)1. 聚类的概念(Clustering)1.1 聚类的概念机器学习按照学习形式可以分为“监督学习”与“无监督学习”，在无监督学习中，训练样本的标记信息未知，即没有“标签”，需要通过对无标记训练样本的学习来揭示数据的内在性质与规律，无监督学习任务中研究最多、应用最广的是聚类。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/Kmeans.jpg">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/KmeansAlgorithm.jpg">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/2.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/3.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/4.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/5.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/6.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/7.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/8.png">
<meta property="og:image" content="http://yoursite.com/images/ML_clustering/9.png">
<meta property="og:updated_time" content="2018-08-07T03:52:50.559Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习之聚类算法">
<meta name="twitter:description" content="机器学习之聚类算法(1)1. 聚类的概念(Clustering)1.1 聚类的概念机器学习按照学习形式可以分为“监督学习”与“无监督学习”，在无监督学习中，训练样本的标记信息未知，即没有“标签”，需要通过对无标记训练样本的学习来揭示数据的内在性质与规律，无监督学习任务中研究最多、应用最广的是聚类。">
<meta name="twitter:image" content="http://yoursite.com/images/ML_clustering/Kmeans.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/05/ML-Cluster/"/>





  <title>机器学习之聚类算法 | Find Qianni</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Find Qianni</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Always be Brave and Enthusiastic</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/ML-Cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Qianni">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/portrait.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Find Qianni">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习之聚类算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-05T17:26:02+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="机器学习之聚类算法-1"><a href="#机器学习之聚类算法-1" class="headerlink" title="机器学习之聚类算法(1)"></a>机器学习之聚类算法(1)</h1><h2 id="1-聚类的概念-Clustering"><a href="#1-聚类的概念-Clustering" class="headerlink" title="1. 聚类的概念(Clustering)"></a>1. 聚类的概念(Clustering)</h2><h3 id="1-1-聚类的概念"><a href="#1-1-聚类的概念" class="headerlink" title="1.1 聚类的概念"></a>1.1 聚类的概念</h3><p>机器学习按照学习形式可以分为“监督学习”与“无监督学习”，在无监督学习中，训练样本的标记信息未知，即<strong>没有“标签”</strong>，需要通过对无标记训练样本的学习来揭示数据的内在性质与规律，无监督学习任务中研究最多、应用最广的是聚类。</p>
<a id="more"></a> 
<p>聚类将数据集中相似的样本归到同一个簇中，将不相似的地样本归到不同的簇中，每个簇可能对应于一些潜在的概念（类别），但这些概念对聚类算法而言事先是未知的。这其中的一系列概念可以表示为：</p>
<p>假定样本集$D=\left\{x_1,x_2,\cdots,x_m \right\}$包含$m$个无标记样本，每个样本$x_i=\left\{x_{i1};x_{i2};\cdots;x_{in} \right\}$是一个$n$维特征向量，则聚类算法将样本集$D$划分为$k$个不相交的簇$\left\{C_l | l=1,2,\cdots,k \right\}$,其中，$C_{l’}\cap_{l’\neq l}C_l=\varnothing $且$D=\cup_{l=1}^kC_l$，相应地，我们用$\lambda_j\in\left\{1,2,\cdots,k\right\}$表示样本$x_j$的簇标记，即$x_j\in C_{\lambda_j}$于是，聚类的结果可用包含$m$个元素的簇标记向量$\lambda=\left(\lambda_1;\lambda_2; \cdots;\lambda_m\right)$表示。</p>
<h3 id="1-2-聚类中的性能度量"><a href="#1-2-聚类中的性能度量" class="headerlink" title="1.2 聚类中的性能度量"></a>1.2 聚类中的性能度量</h3><p>聚类分析试图将相似对象划为同一簇，不相似的对象归到不同簇，而“相似”主要依靠性能度量。</p>
<p>聚类的性能度量（有效性指标）是用于评估聚类效果的好坏的或作为聚类过程的优化目标的。聚类性能度量大致可以分为两类，一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”。</p>
<h4 id="1-2-1-外部指标"><a href="#1-2-1-外部指标" class="headerlink" title="1.2.1 外部指标"></a>1.2.1 外部指标</h4><p>对数据集$D=\left\{x_1,x_2,\cdots,x_m \right\}$，假定通过聚类给出的簇划分为$C=\left\{C_1,C_2,\cdots,C_k \right\}$参考模型给出的簇划分为$C^<em>=\left\{C_1^</em>,C_2^<em>,\cdots,C_s ^</em>\right\}$。相应地，令$\lambda$与$\lambda^<em>$分别表示$C$与$C^</em>$对应的簇标记向量，将样本两两配对考虑，则可定义：</p>
<script type="math/tex; mode=display">
a=\left|SS\right|, SS=\left\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
b=\left|SD\right|, SD=\left\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*\neq\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
c=\left|DS\right|,DS=\left\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda_i^*=\lambda_j^*,i<j\right\}</script><script type="math/tex; mode=display">
d=\left|DD\right|, SS=\left\{(x_i,x_j)|\lambda_i\neq\lambda_j,\lambda_i^*\neq\lambda_j^*,i<j\right\}</script><p>其中集合$SS$为在$C$中隶属于相同簇且在$C^*$中也隶属于相同簇的样本对，其余同理可以推得，此外$a+b+c+d=\frac{m(m-1)}{2}$</p>
<p>通过以上定义得到聚类性能度量的外部指标：</p>
<ul>
<li>Jaccard系数 $JC=\frac{a}{a+b+c}$</li>
<li>FM指数 $FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}$</li>
<li>Rand指数 $RI=\frac{2(a+d)}{m(m-1)}$</li>
</ul>
<h4 id="1-2-2-内部指标"><a href="#1-2-2-内部指标" class="headerlink" title="1.2.2 内部指标"></a>1.2.2 内部指标</h4><p>根据簇划分$C=\left\{C_1,C_2,\cdots,C_k \right\}$，定义：</p>
<script type="math/tex; mode=display">
avg(C)=\frac{2}{\left|C\right|(\left|C\right| - 1)}\sum_{1\leq i<j\leq\left|C\right|}dist(x_i,x_j)</script><script type="math/tex; mode=display">
diam(C)=\max_{1\leq i<j\leq\left|C\right|}dist(x_i,x_j)</script><script type="math/tex; mode=display">
d_\min (C_i,C_j)=\min_{x_i\in C_i,x_j \in C_j}dist(x_i,x_j)</script><script type="math/tex; mode=display">
d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)</script><p>$dist$为样本距离计算公式，$\mu$代表簇$C$的中心点，$avf(C)$对应簇$C$内样本间的平均距离，$diam(C)$对应簇内$C$内样本间的最远距离，$d_\min (C_i,C_j)$为两个簇最近样本间的距离$d_{cen}(C_i,C_j)$为两簇中心点间的距离。</p>
<p>通过以上可以导出以下内部指标：</p>
<ul>
<li>DBI指数 $DBI =\frac{1}{k}\sum_{i=1}^k\max_{j\neq i}\left( \frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\right)$</li>
<li>Dunn指数 $DI=\min_{1\leq i \leq k}\left\{\min_{j\neq i}\left(\frac{d_\min(C_i,C_j)}{\max_{1\leq l \leq k}diam(C_l)}\right)\right\}$</li>
</ul>
<p>DBI的值越小越好，DI的值越大越好。</p>
<h3 id="1-3-聚类中的距离"><a href="#1-3-聚类中的距离" class="headerlink" title="1.3 聚类中的距离"></a>1.3 聚类中的距离</h3><p>性能度量中的内部指标涉及到“距离”的度量，数据的属性可以分为“有序属性”（如{1,2,3}可以直接衡量出各个量之间的距离）与“无序属性”（如{飞机，火车，轮船}不能直接刻画各个量之间的距离）。</p>
<p>其中<strong>“有序属性”</strong>可以通过<strong>闵可夫斯基距离</strong>计算：</p>
<script type="math/tex; mode=display">
dist_{mk}(x_i,x_j)=\left(\sum_{u=1}^n \left|x_{iu}-x_{ju}\right|^p\right)^\frac{1}{p}</script><p>p=1时，为曼哈顿距离；p=2时为欧式距离；此外当样本空间中不同属性的重要性不同时，可以使用“加权距离”进行计算。</p>
<p><strong>“无序属性”</strong>可以通过<strong>VDM</strong>(value Difference Metric)计算：</p>
<script type="math/tex; mode=display">
VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p</script><p>其中$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示在第$i$个样本簇中在属性$u$上取值为$a$的样本数，$k$为样本簇数。</p>
<p>对于<strong>同时存在“有序属性”与“无序属性”</strong>的情况，可以采用<strong>混合距离定义</strong>，其中有$n_c$个有序属性，$n-n_c$个无序属性：</p>
<script type="math/tex; mode=display">
MinkovDM_p(x_i,x_j)=\left( \sum_{u=1}^n \left|x_{iu}-x_{ju}\right|^p+ \sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\right)^\frac{1}{p}</script><p>距离度量需要满足如非负性、同一性、对称性、直递性（两边之和大于第三边）等性质，但相似度度量的过程中使用的距离未必一定要满足距离度量的所有基本性质。</p>
<h3 id="1-4-聚类的分类"><a href="#1-4-聚类的分类" class="headerlink" title="1.4 聚类的分类"></a>1.4 聚类的分类</h3><p>看过几个版本的聚类的分类方法，基本是一致的，但也有一点点差异，此处选取周志华《机器学习》一书中的分类方法。</p>
<ul>
<li><strong>原型聚类</strong>：假设聚类结构能通过一组<strong>原型</strong>刻画，算法一般先对原型进行初始化，然后对原型进行迭代更新求解。代表方法包括<strong>k-means</strong>；LVQ；<strong>GMM</strong>等。</li>
<li><strong>密度聚类</strong>：基于密度的方法的特点是不依赖于距离，而是<strong>依赖于密度</strong>，从而克服基于距离的算法只能发现“球形”聚簇的缺点。其核心思想在于只要一个区域中点的密度大于某个阈值，就把它加到与之相近的聚类中去。 通常情况，密度聚类算法从样本密度的角度来考察样本之间的<strong>可连接性</strong>，并基于<strong>可连接样本不断扩展聚类簇</strong>以获得最终的结果。代表方法包括<strong>DBSCAN</strong>、OPTICS，DENCLUE，WaveCluster等。</li>
<li><strong>层次聚类</strong>：试图在不同层次对数据集进行划分，从而形成树型的聚类结构，存在“自底而上”的聚合策略，与“自顶而下”的分拆策略。在“自底向上”方案中，初始时每个数据点组成一个单独的组，在接下来的迭代中，按一定的距离度量将相互邻近的组合并成一个组，直至所有的记录组成一个分组或者满足某个条件为止。 代表方法包括：AGNES、BIRCH，CURE，CHAMELEON等。</li>
</ul>
<h2 id="2-原型聚类——K-均值聚类算法-K-means"><a href="#2-原型聚类——K-均值聚类算法-K-means" class="headerlink" title="2.原型聚类——K-均值聚类算法(K-means)"></a>2.原型聚类——K-均值聚类算法(K-means)</h2><h3 id="2-1-过程及算法"><a href="#2-1-过程及算法" class="headerlink" title="2.1 过程及算法"></a>2.1 过程及算法</h3><p><img src="/images/ML_clustering/Kmeans.jpg" alt="Kmeans "></p>
<p>如上图所示，是K均值聚类的原理。对数据集$D=\left\{x_1,x_2,\cdots,x_m \right\}$，K-均值算法针对聚类所得簇划分$C=\left\{C_1,C_2,\cdots,C_k \right\}$最小化平方误差：</p>
<script type="math/tex; mode=display">
E=\sum_{i=1}^k \sum_{x\in C_i}\|x-\mu_i \|_2^2</script><p>其中k是用户给定的簇的个数，$\mu_i$是簇$C_i$的均值向量，即簇的“质心”。找到E的最小值是一个<a href="https://en.wikipedia.org/wiki/NP-hardness" target="_blank" rel="noopener">NP困难问题</a>，k-均值算法采用<a href="https://en.wikipedia.org/wiki/Greedy_algorithm" target="_blank" rel="noopener">贪心策略</a>，通过迭代优化近似求解，算法首先对于质心（均值向量）进行初始化，随机确定k个初始点作为质心，然后将数据集中的每个点寻找距离其最近的质心，并将该点分给该质心代表的簇，之后更新簇的质心（均值向量）为该簇现有所有点的平均值。</p>
<p>伪代码过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为起始质心（可以在样本中随机选择k个）</span><br><span class="line">当任意一个点的簇分配结果发生改变时</span><br><span class="line">	对数据集中的每个数据点</span><br><span class="line">		对每个质心</span><br><span class="line">			计算质心与数据点间的距离</span><br><span class="line">		将数据点分配到距离其最近的簇</span><br><span class="line">	对每个簇，计算簇中所有点的均值向量，并将值作为质心</span><br></pre></td></tr></table></figure>
<p><img src="/images/ML_clustering/KmeansAlgorithm.jpg" alt="KmeansA伪代码"></p>
<p>使用Python建立k-means过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#导入tab分隔文件</span></span><br><span class="line">    dataMat = []                <span class="comment">#assume last column is target value</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float,curLine)) <span class="comment">#此处map前加一个list才保证不出错</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span>       <span class="comment">#计算点之间的欧式距离</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment">#la.norm(vecA-vecB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span>        <span class="comment">#随机选取质心</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))<span class="comment">#create centroid mat</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment">#create random cluster centers, within bounds of each dimension</span></span><br><span class="line">        minJ = min(dataSet[:,j]) </span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span><span class="comment">#kmeans建立，输入数据集、簇个数</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]<span class="comment">#样本个数</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>)))<span class="comment">#存放聚类的类别和距离值</span></span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:<span class="comment">#当质心有所改变时持续循环</span></span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):<span class="comment">#对于每个数据点分配最近的质心</span></span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> (centroids)</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):<span class="comment">#更新质心</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]<span class="comment">#get all the point in this cluster</span></span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment">#assign centroid to mean </span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>
<h3 id="2-2-优缺点及适用性分析"><a href="#2-2-优缺点及适用性分析" class="headerlink" title="2.2 优缺点及适用性分析"></a>2.2 优缺点及适用性分析</h3><p>优点：便于实现</p>
<p>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</p>
<h2 id="3-二分K-均值算法-bisecting-K-means"><a href="#3-二分K-均值算法-bisecting-K-means" class="headerlink" title="3. 二分K-均值算法(bisecting K-means)"></a>3. 二分K-均值算法(bisecting K-means)</h2><p>由于传统的KMeans算法的聚类结果<strong>易受到初始聚类中心点选择的影响</strong>，因此在传统的KMeans算法的基础上进行算法改进，对初始中心点选取比较严格，各中心点的距离较远，这就避免了初始聚类中心会选到一个类上，一定程度上<strong>克服了算法陷入局部最优状态</strong>。 二分KMeans(Bisecting KMeans)算法的主要思想是：首先将<strong>所有点作为一个簇</strong>，然后将该簇<strong>一分为二</strong>。之后<strong>选择能最大限度降低聚类代价函数（也就是<code>误差平方和</code>SSE）的簇划分为两个簇</strong>。以此进行下去，<strong>直到簇的数目等于用户给定的数目k为止</strong>。以上隐含的一个原则就是：因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以我们就需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = np.mat(np.zeros((m,<span class="number">2</span>)))</span><br><span class="line">    centroid0 = np.mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">    centList =[centroid0] <span class="comment">#create a list with one centroid</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):<span class="comment">#calc initial Error</span></span><br><span class="line">        clusterAssment[j,<span class="number">1</span>] = distMeas(np.mat(centroid0), dataSet[j,:])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):</span><br><span class="line">        lowestSSE = np.inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]<span class="comment">#get the data points currently in cluster i</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)</span><br><span class="line">            sseSplit = sum(splitClustAss[:,<span class="number">1</span>])<span class="comment">#compare the SSE to the currrent minimum</span></span><br><span class="line">            sseNotSplit = sum(clusterAssment[np.nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"sseSplit, and notSplit: "</span>,sseSplit,sseNotSplit)</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = len(centList) <span class="comment">#change 1 to 3,4, or whatever</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the bestCentToSplit is: '</span>,bestCentToSplit)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the len of bestClustAss is: '</span>, len(bestClustAss))</span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>]<span class="comment">#replace a centroid with two best centroids </span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>])</span><br><span class="line">        clusterAssment[np.nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss<span class="comment">#reassign new clusters, and SSE</span></span><br><span class="line">    <span class="keyword">return</span> np.mat(centList), clusterAssment</span><br></pre></td></tr></table></figure>
<h2 id="4-K-均值算法实战：对地图上的点进行聚类"><a href="#4-K-均值算法实战：对地图上的点进行聚类" class="headerlink" title="4.K-均值算法实战：对地图上的点进行聚类"></a>4.K-均值算法实战：对地图上的点进行聚类</h2><p>这次没有选择《机器学习实战》课本上的案例，而是选取了2016.12.22周四00:00-00:30之间上海市出租车订单数据中所有正常数据的订单起始点进行聚类（总共137个数据点，数据来源是上学期交通地理信息系统课程）。此外还使用了上海市的路网shp文件，借助geopandas、numpy、matplotlib、pandas等模块，完成这些起始点的聚类，通过此过程我们可以看到热点地区的分布情况。</p>
<p>通过调用clusterOrders(numClust)即可进行进行以上聚类过程，此部分源代码如下，此外还需导入randCent()，kMeans()，biKmeans()函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distSLC</span><span class="params">(vecA, vecB)</span>:</span><span class="comment">#由经纬度转化为距离</span></span><br><span class="line">    a = np.sin(vecA[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * np.sin(vecB[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>)</span><br><span class="line">    b = np.cos(vecA[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * np.cos(vecB[<span class="number">0</span>,<span class="number">1</span>]*np.pi/<span class="number">180</span>) * \</span><br><span class="line">                      np.cos(np.pi * (vecB[<span class="number">0</span>,<span class="number">0</span>]-vecA[<span class="number">0</span>,<span class="number">0</span>]) /<span class="number">180</span>)</span><br><span class="line">    <span class="keyword">return</span> np.arccos(a + b)*<span class="number">6371.0</span> </span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd</span><br><span class="line">route=gpd.read_file(<span class="string">'SHMAP_Main.shp'</span>)<span class="comment">#导入路网shp文件</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clusterOrders</span><span class="params">(numClust=<span class="number">5</span>)</span>:</span>      <span class="comment">##对订单进行聚类</span></span><br><span class="line">    datList = []</span><br><span class="line">    data = pd.read_csv(<span class="string">'Thursdaymidnight.csv'</span>)  <span class="comment">##导入订单数据</span></span><br><span class="line">    location = data[[<span class="string">'s_long'</span>,<span class="string">'s_la'</span>]] <span class="comment">#使用订单的起点经纬度数据</span></span><br><span class="line">    inner=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(location.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">            a=location.iloc[i][j]</span><br><span class="line">            inner.append(a)</span><br><span class="line">        datList.append(inner)</span><br><span class="line">        inner=[]</span><br><span class="line">    datMat = np.mat(datList)    </span><br><span class="line">    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC) <span class="comment">#调用二分K-均值算法进行聚类</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">9</span>,<span class="number">15</span>))</span><br><span class="line">    rect=[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.8</span>,<span class="number">0.8</span>]</span><br><span class="line">    scatterMarkers=[<span class="string">'s'</span>, <span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'8'</span>, <span class="string">'p'</span>, \</span><br><span class="line">                    <span class="string">'d'</span>, <span class="string">'v'</span>, <span class="string">'h'</span>, <span class="string">'&gt;'</span>, <span class="string">'&lt;'</span>]</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax0=fig.add_axes(rect, label=<span class="string">'ax0'</span>, **axprops)</span><br><span class="line">    route.plot(ax=ax0,color=<span class="string">'darkgrey'</span>,alpha=<span class="number">0.2</span>)  <span class="comment">#绘制路网</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numClust):</span><br><span class="line">        ptsInCurrCluster = datMat[np.nonzero(clustAssing[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">        markerStyle = scatterMarkers[i % len(scatterMarkers)]</span><br><span class="line">        ax0.scatter(ptsInCurrCluster[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], ptsInCurrCluster[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=markerStyle, s=<span class="number">60</span>) <span class="comment">#绘制订单数据点</span></span><br><span class="line">    ax0.scatter(myCentroids[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], myCentroids[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=<span class="string">'+'</span>, s=<span class="number">300</span>,color=<span class="string">'k'</span>) <span class="comment">#绘制质心</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>尝试簇的数目由2至9，得到的聚类结果如下图所示：</p>
<p><img src="/images/ML_clustering/2.png" alt="Kmeans-1"><br><img src="/images/ML_clustering/3.png" alt="Kmeans-2"><br><img src="/images/ML_clustering/4.png" alt="Kmeans-3"><br><img src="/images/ML_clustering/5.png" alt="Kmeans-4"><br><img src="/images/ML_clustering/6.png" alt="Kmeans-5"><br><img src="/images/ML_clustering/7.png" alt="Kmeans-6"><br><img src="/images/ML_clustering/8.png" alt="Kmeans-7"><br><img src="/images/ML_clustering/9.png" alt="Kmeans-8"></p>
<p>此案例只应用了非常少量的一部分订单数据，在应用于量较大的订单数据时，我们可以不绘制各个订单起始点，而是将每一簇的数目与簇质心‘+’符号的尺寸产生关联，使热点区域的显示更为直观。</p>
<p>关于原型聚类中的GMM高斯混合模型聚类、密度聚类与层次聚类由于内容较多，推导较为复杂，本篇内就不再详细叙述，GMM大概会与EM算法一起写一篇吧！</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>周志华《机器学习》第9章<br>《机器学习实战》第10章<br>很多很多没有记录下来的博客</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Clustering/" rel="tag"># Clustering</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/29/ML-1to6task/" rel="next" title="机器学习报告补齐（包括KNN、决策树、朴素贝叶斯、logistic回归、TensorFlow、SVM）">
                <i class="fa fa-chevron-left"></i> 机器学习报告补齐（包括KNN、决策树、朴素贝叶斯、logistic回归、TensorFlow、SVM）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/19/ML-PCA/" rel="prev" title="机器学习之主成分分析">
                机器学习之主成分分析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNjY5NC8xMzIyOQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/portrait.jpg"
                alt="Wang Qianni" />
            
              <p class="site-author-name" itemprop="name">Wang Qianni</p>
              <p class="site-description motion-element" itemprop="description">It is a place for Qianni to record and learn.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习之聚类算法-1"><span class="nav-number">1.</span> <span class="nav-text">机器学习之聚类算法(1)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-聚类的概念-Clustering"><span class="nav-number">1.1.</span> <span class="nav-text">1. 聚类的概念(Clustering)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-聚类的概念"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 聚类的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-聚类中的性能度量"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 聚类中的性能度量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-外部指标"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.2.1 外部指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-内部指标"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">1.2.2 内部指标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-聚类中的距离"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 聚类中的距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-聚类的分类"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 聚类的分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-原型聚类——K-均值聚类算法-K-means"><span class="nav-number">1.2.</span> <span class="nav-text">2.原型聚类——K-均值聚类算法(K-means)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-过程及算法"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 过程及算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-优缺点及适用性分析"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 优缺点及适用性分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-二分K-均值算法-bisecting-K-means"><span class="nav-number">1.3.</span> <span class="nav-text">3. 二分K-均值算法(bisecting K-means)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-K-均值算法实战：对地图上的点进行聚类"><span class="nav-number">1.4.</span> <span class="nav-text">4.K-均值算法实战：对地图上的点进行聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">1.5.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Qianni</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
